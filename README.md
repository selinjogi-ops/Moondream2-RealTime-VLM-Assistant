# ğŸŒ™ Moondream2 Real-Time Vision Language Assistant

An assistive real-time AI system powered by **Moondream-2 (Vision Language Model)** that interprets live visual input and generates natural language responses with multilingual voice output.

Designed as an educational and accessibility-focused initiative to support visually impaired individuals in understanding their surroundings independently.

---

## ğŸš€ Features

### ğŸ¥ Real-Time Scene Understanding

* Live webcam integration
* Instant scene descriptions
* Object and environment awareness
* Continuous visual interpretation

### ğŸ“ Text Reading (OCR + VLM)

* Detects and reads visible text
* Works on books, signboards, labels
* Supports translation into multiple languages

### ğŸ’° Indian Currency Recognition

* Detects â‚¹10, â‚¹20, â‚¹50, â‚¹100, â‚¹200, â‚¹500 notes
* Announces denomination via voice output
* Designed for accessibility use cases

### ğŸŒ Multilingual Voice Output

Supports:

* English
* Hindi
* Kannada

Includes real-time translation and speech synthesis.

### ğŸ§¾ Evidence Logging

* Saves captured frames
* Stores AI-generated descriptions
* Useful for debugging and demonstrations

---

## â™¿ Accessibility Impact

This system is built as an assistive AI tool that enables visually impaired users to:

* Understand surroundings through audio descriptions
* Read printed text independently
* Identify currency without assistance
* Interact in multiple languages

---

## ğŸ—ï¸ System Architecture

Moondream-2 integrates:

* **Vision Encoder** â†’ Converts image into visual embeddings
* **Language Model** â†’ Generates contextual text responses

Core workflow:

1. Capture frame via OpenCV
2. Encode image using `encode_image()`
3. Generate response using `answer_question()`
4. Translate (optional)
5. Convert text â†’ speech
6. Play audio output

---

## ğŸ§  Model Information

* **Model:** `vikhyatk/moondream2`
* **Type:** Vision-Language Model (VLM)
* **Parameters:** ~1.8B
* **Release Year:** 2024
* **Revision Used:** `2024-08-26`
* **Framework:** HuggingFace Transformers
* **Model Class:** `AutoModelForCausalLM`

### Why Moondream-2?

* Lightweight (~1.8B parameters)
* Runs locally (CPU compatible)
* Fast inference
* Suitable for edge AI deployment
* Open-source (Apache-2.0)

---

## ğŸ› ï¸ Tech Stack

| Component         | Technology               |
| ----------------- | ------------------------ |
| Language          | Python                   |
| VLM               | Moondream-2              |
| Deep Learning     | PyTorch                  |
| Model Integration | HuggingFace Transformers |
| Computer Vision   | OpenCV                   |
| Speech Output     | gTTS / pyttsx3           |
| Translation       | Google Translator        |

---

## ğŸ’» Hardware Requirements

Minimum:

* 4GB RAM
* CPU support

Recommended:

* 8GB RAM
* NVIDIA GPU (for faster inference)

---

## ğŸ¯ Project Objectives

* Demonstrate real-time Vision-Language AI
* Build an accessibility-focused AI assistant
* Introduce students to applied AI systems
* Enable multilingual AI interaction
* Bridge computer vision and natural language processing

---

## ğŸ« Educational Implementation

Implemented during a mentoring session at:

**Indus International School, Bangalore (IISB)**

Students gained hands-on exposure to:

* Real-time AI system development
* Vision Language Models
* Practical debugging
* Applied AI problem solving

---

## ğŸ“ˆ Future Improvements

* Mobile application integration
* Edge deployment optimization
* Expanded language support
* Enhanced object detection
* Wearable device integration
* Navigation assistance for visually impaired users

---

## ğŸ‘¨â€ğŸ’» Author

**Selin Jogi Chittilappilly**
B.Voc Mathematics & Artificial Intelligence
AI/ML Enthusiast | Computer Vision | Vision Language Models

---

## ğŸ™ Acknowledgment

* Niya C Anto â€“ Mentor
* Neeraj PM â€“ Head of Engineering



